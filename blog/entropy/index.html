<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8"/>
  <title>Entropy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- RSS feed -->
  <link rel="alternate" type="application/rss+xml" title="Penn Data Science Group"
  href="http://penndsg.com/feed.xml">

  <!-- Customized Bootstrap + Font Awesome + Solarized -->
  <!-- Font Raleway from Google fonts -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:regular,bold" rel="stylesheet">
  <link href="/css/style.css" rel="stylesheet" media="screen">

  <link type="text/css" rel="stylesheet" href="/lightslider/css/lightslider.css" />
  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/favicons/manifest.json">
  <link rel="mask-icon" href="/favicons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/favicons/favicon.ico">
  <meta name="msapplication-config" content="/favicons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-71898636-2', 'auto');
    ga('send', 'pageview');

  </script>
  <!-- END Google Analytics -->

  <!-- jQuery -->
  <script src="/js/jquery.min.js"></script>

  <!-- Bootstrap -->
  <script src="/js/transition.js"></script>
  <script src="/js/collapse.js"></script>

  <script src="/lightslider/js/lightslider.js"></script>

  <!-- Custom script to put footer at bottom even if post is short -->
  <!-- also controls color of footer logo on mouseover -->
  <script src="/js/footer.js"></script>

  <!-- Smooth Scrolling -->
  <script src="/js/smooth-scroll.min.js"></script>

  <!-- Math Rendering -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head>

<body>


  
  <div id="header">
  
    <div>
    
    <nav class="navbar navbar-default dark-bkg" role="navigation">
    
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-element" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">
            PDSG
          </a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-collapse-element">
          <ul class="nav navbar-nav navbar-right">
            
            <li>
            
              <a href="/events/">Events</a>
            </li>
            
            <li>
            
              <a href="/gallery/">Gallery</a>
            </li>
            
            <li>
            
              <a href="/resources/">Resources</a>
            </li>
            
            <li>
            
              <a href="/members/">Members</a>
            </li>
            
            <li>
            
              <a href="/join/">Join</a>
            </li>
            
            <li>
            
              <a href="/collaborate/">Collaborate</a>
            </li>
            
            <li class="active">
            
              <a href="/blog/">Blog</a>
            </li>
            
            <li>
            
              <a href="/contact/">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
  </div>

  
  </div>

  <div id="content-wrapper">
    <div class="container blog-post">
  
  <div class="text-center">
    <h1>Entropy</h1>
  </div>
  <hr>
  

  <div class="row">
    <div class="col-md-1"></div>
    <div class="col-md-10">
      
      
      
        
          
          
          
          
      <div class="row">
        
        <div class="col-md-12 text-center" >
          <a class="off" href="/members/roshan-santhosh/">
            <img src="/images/members/roshan-santhosh.jpg" class="img-thumbnail" style="width: 150px;">
          </a>
        </div>
        
        <div class="col-md-12 text-center">
          <a class="off" href="/members/roshan-santhosh/">
            <h4>Roshan Santhosh</h4>
          </a>
        </div>
      </div>

      <div class="spacer"></div>

      <div class="post">
        <p>Entropy as a concept is rarely mentioned while discussing Machine Learning.
However, the concept forms the basis for a lot of the Machine Learning
algorithms that we see today.</p>

<p>In terms of a formal definition, Entropy <script type="math/tex">H(X)</script> of a variable is defined by <script type="math/tex">\sum_{x} p(x)*log_{2} \left ( \frac{1}{p(x)} \right )</script></p>

<p>But there are better ways to grasp the concept of Entropy.</p>

<h2 id="definition-1">Definition 1:</h2>

<p>Amount of information gained on receiving a sample from a distribution. This information is measured in bits (if log is taken to base 2)</p>

<p><strong>Fair coin with two values with equal probability</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
H(X) =&\; p(x_{i})log_{2} \left ( \frac{1}{p(x_{i})} \right ) +
p(x_{j})log_{2} \left ( \frac{1}{p(x_{j})} \right ) \\

  H(X) =&\; 0.5log_{2}\left ( 2 \right ) + 0.5*log_{2}\left ( 2 \right ) \\

  H(X) =&\; 1 \\
\end{align*} %]]></script>

<p><strong>Throwing M fair coins</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  H(X) =&\; \frac{1}{2^{M}}*\log_{2}{2^{M}} + \frac{1}{2^{M}}\log_{2}{2^{M}} +
\frac{1}{2^{M}}\log_{2}{2^{M}} + ..... (2^{M}  \textrm{times}) \\

  H(X) =&\; \left\{\frac{M}{2^{M}}\right\}2^{M} \\

  H(X) =&\; M
\end{align*} %]]></script>

<p><strong>Fair dice with M faces</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  H(X) =&\; \left [ \frac{1}{M}log_{2}\left ( M \right ) \right ]M \\

  H(X) =&\; log_{2}\left ( M \right )
\end{align*} %]]></script>

<p><strong>Event with probability distribution  :</strong> <script type="math/tex">\left\{\frac{1}{2} , \frac{1}{4} ,
\frac{1}{8} , \frac{1}{8}\right\}</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  H(X) =&\;  \frac{1}{2}log_{2}\left ( 2 \right ) + \frac{1}{4}log_{2}\left ( 4 \right ) + \frac{1}{8}log_{2}\left ( 8 \right ) + \frac{1}{8}log_{2}\left ( 8 \right ) \\
  H(X) =&\; 1.75
\end{align*} %]]></script>

<p>For the last example, the entropy of 1.75 implies that every time we receive information about a sample from this distribution, we get 1.75 bits of information on average.</p>

<p>Another interesting way of looking at entropy is by thinking of the number of bits required on average to convey the information of an event from this distribution to the entity receiving this information.</p>

<p>We start by assigning each event a specific code, the codes being 0,10,110,111
for each of the events respectively. Therefore, the average code length of
messages would be <script type="math/tex">0.5 \times 1 + 0.25 \times 2 + 0.125 \times 3 + 0.125
\times 3</script> which equals <script type="math/tex">1.75</script>. Therefore on average 1.75 bits of information is sent each time to
communicate an event from this variable.</p>

<p>This is the same value that we got from the entropy formula. It goes as such that the entropy of a distribution is the minimum amount of bits required on average to communicate the samples of the variable. Therefore the encoding used above where the event with probability 0.5 is assigned the code 0, the event with probability 0.25 is assigned the code 10 and so on, is the most efficient encoding possible as the average code length equals the entropy of the distribution. Any other encoding would result in an average code length greater than 1.75</p>

<p>Take another example where the an event <script type="math/tex">E</script> has two outcomes <script type="math/tex">A</script> and <script type="math/tex">B</script> with probabilities of 1 and 0. In this case, the entropy of this distribution turns out to be 0. This implies that with every new sample from the distribution, we receive no new information. This is true as we know for sure that the sample is the outcome A.</p>

<p><strong>NOTE</strong>
An thing to notice here is that the example taken above is an ideal case where the probabilities are all powers of 2 and hence the entropy calculated and the average code length are the same. In cases with arbitrary probabilities, the average code length would not be equal (would be greater than actually) to the entropy of the distribution.</p>

<p>Lets take an example of a variable with a distribution of <script type="math/tex">\left \langle 0.3,0.7 \right \rangle</script>.</p>

<p>The code lengths for the two events as given by the entropy equation are :</p>

<p>Event A with probability 0.3 : <script type="math/tex">log_{2}\frac{1}{0.3} = 1.73</script></p>

<p>Event B with probability 0.7 : <script type="math/tex">log_{2}\frac{1}{0.7} = 0.51</script></p>

<p>The entropy of the distribution : <script type="math/tex">0.3 \times 1.73 + 0.7 \times 0.51 = 0.876</script></p>

<p>The encoding system that we have used till now (Hoffman encoding) does not
handle fraction bits quite that well. Therefore we end up using single bits  for
both the events. To better understand how we can still achieve the average code
length speculated by the entropy of the distribution, check the section on <strong>Fractional Bits</strong> in <a href="http://colah.github.io/posts/2015-09-Visual-Information/">this blog</a>. The basic idea here is to increase the number of
events encoded for with a single code. Right now we have one code for event A
and one for event B. Instead have codes for two events i.e a codes for AA, AB,
BA and BB. The section in the above mentioned blog shows how this approach
reduces the average code for each event from the current 1 to approximately 0.9.
By theory, if we keep on increasing the number of events encoded for by a code,
we would be able to reach the minimum length given by the entropy (0.876).</p>

<h2 id="definition-2">Definition 2:</h2>

<p>Entropy can also be seen as a measure of uncertainty in a variable. It defines how ‘pure’ or ‘homogenous’ the distributions are.</p>

<p>As seen in the case of a fair coin, a uniform distribution has an entropy of 1, which is the maximum achievable for a distribution with two outcomes. This is true for any distribution with n number of outcomes, with the uniform distribution having the highest entropy. This runs along with the second definition of Entropy since a uniform distribution would have the highest uncertainty.</p>

<p>The following distributions and their entropies would make this definition a bit
more clear.</p>

<p><img src="/images/blog/entropy-dist-1.jpg" alt="Distribution 1" />
<img src="/images/blog/entropy-dist-2.jpg" alt="Distribution 2" />
<img src="/images/blog/entropy-dist-3.jpg" alt="Distribution 3" />
<img src="/images/blog/entropy-dist-4.jpg" alt="Distribution 4" /></p>

<p>As can be seen from the above examples, the <strong>farther the distribution is from a
uniform distribution, lesser is the uncertainty and hence lesser is the
entropy.</strong></p>

<h2 id="information-gain">Information Gain</h2>

<p>Information Gain as a concept is commonly used in decision trees as a measure for determining the relevance of a particular variable. In simple terms, it refers to the gain in information or reduction in entropy when a variable is conditioned on another variable.</p>

<script type="math/tex; mode=display">IG (T,a) = H(T) - H(T|a)</script>

<p>The following example on the splitting of a node in a decision tree would better
illustrate the application of Information Gain.</p>

<p><img src="/images/blog/entropy-dist-5.jpg" alt="Distribution 5" /></p>

<p>Lets consider the below distribution consisting of two target classes, denoted by circles and squares. Now the task of the decision tree is to differentiate between the two target classes using the given variables. In this case, the variables provided are the color of the samples as well as the presence/absence of borders in the samples.</p>

<p>Therefore, we evaluate the classifying ability of both these variables by
estimating the Information Gain for both the variables.</p>

<p><img src="/images/blog/entropy-splitting-on-border.jpg" alt="Splitting on Border" /></p>

<p>In the above case, the distribution is split based on the presence/absence of a border in the samples. In the resulting split, we get two child nodes with entropies of 0.7642 and 0.8113. On calculating the information gain for the split, we get</p>

<script type="math/tex; mode=display">IG(X|border) = 0.9975 - \left[ \frac{9}{17}*0.7642 + \frac{8}{17} * 0.8113 \right]</script>

<script type="math/tex; mode=display">IG(X|border) = 0.2117</script>

<p>Therefore the information gained or the overall reduction in entropy of the distribution by splitting on the border variable is 0.2117 bits.</p>

<p><img src="/images/blog/entropy-splitting-on-color.jpg" alt="Splitting on Color" /></p>

<p>The distribution is now split based on the color of the samples. In the resulting split, we get two child nodes with entropies of 0.9911 and 1.0 . On calculating the information gain for the split, we get</p>

<script type="math/tex; mode=display">IG(X|color) = 0.9975 - \left [ \frac{9}{17}*0.9911   +  \frac{8}{17}*1.0
\right ]</script>

<script type="math/tex; mode=display">latex IG(X|color) = 0.0022</script>

<p>The information gained or the overall reduction in entropy of the distribution by splitting on the color variable is 0.0022 bits.</p>

<p>Since the border variable results in higher information gain, it is a better candidate to split the node on. The above result can easily be verified visually. On observing the child nodes created by splitting on the border variable, we can observe that the left child node has a strong concentration of squares while the right child node has a strong concentration of circles. This shows that the variable has been able to differentiate the two classes well.
However, in the case of the child nodes created by splitting on color, both the child nodes have an equal mix of both the target classes and hence proves to be a poor differentiator.</p>

<p>This visual difference that we observe in the classification power of variables
is quantified using Information Gain. <strong>The higher the ‘purity’ of the resulting child nodes, the higher the Information Gain of the splitting variable.</strong></p>

      </div>
    </div>
    <div class="col-md-1"></div>
  </div>

  <div class="bigspacer"></div>
  <div class="spacer"></div>

</div>

  </div>

  <div id="footer">
    <nav class="navbar navbar-inverse text-center" role="navigation">
      <div class="spacer"></div>
      <div class="container">
        <ul class="nav navbar-nav footernav">
          <li>
            <span class="fa-stack fa-lg">
              <a href="https://www.facebook.com/PennDSG/">
                <i class="fa fa-facebook fa-stack-2x"></i>
              </a>
            </span>
            <span class="fa-stack fa-lg">
              <a href="https://www.linkedin.com/groups/8583138">
                <i class="fa fa-linkedin fa-stack-2x"></i>
              </a>
            </span>
            <span class="fa-stack fa-lg">
              <a href="https://join.slack.com/t/pdsg/signup">
                <i class="fa fa-slack fa-stack-2x"></i>
              </a>
            </span>
            <!-- <span class="fa-stack fa-lg"> -->
            <!--   <a href="https://github.com/penndsg"> -->
            <!--     <i class="fa fa-github-square fa-stack-2x"></i> -->
            <!--   </a> -->
            <!-- </span> -->
            <!-- <span class="fa-stack fa-lg"> -->
            <!--   <a href="https://www.youtube.com/channel/UCLsuOKhFp2lG21BTvWbcbeQ"> -->
            <!--     <i class="fa fa-youtube-square fa-stack-2x"></i> -->
            <!--   </a> -->
            <!-- </span> -->
          </li>
        </ul>
      </div>
      <div class="spacer"></div>
    </nav>
  </div>
</body>

<!-- Initialize the Smooth Scrolling function -->
<script>
  smoothScroll.init();
</script>

</html>
